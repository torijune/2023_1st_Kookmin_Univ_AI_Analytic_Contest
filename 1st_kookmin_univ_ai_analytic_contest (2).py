# -*- coding: utf-8 -*-
"""1st_Kookmin_Univ_AI_Analytic_Contest

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KAj8ZWBuWo2ZAfY4wOiG3sNy7Xnjm-nf

# **Dataset Info.**

**apply_train.csv**

이력서가 채용 공고에 실제 지원한 관계 목록 (히스토리)

**이력서 관련 데이터**

resume.csv

resume_certificate.csv

resume_education.csv

resume_language.csv

**채용공고 관련 데이터**

recruitment.csv
company.csv

sample_submission.csv [파일]

- 제출 양식

resume_seq : 추천을 진행할 이력서 고유 번호

recruitment_seq : 이력서에 대해 추천한 채용 공고 고유 번호

resume.csv에 존재하는 모든 resume_seq에 대해서 5개의 채용 공고를 추천해야 합니다.

해당 이력서에서 실제 지원이 이루어졌던 채용 공고는 추천하지 않습니다.

중복된 채용 공고를 추천하거나, 5개가 아닌 개수의 채용 공고를 추천하는 경우 제출이 불가능합니다.

# **참고할만한 문헌**

**유사도를 활용한 맞춤형 보험 추천 시스템**
 - https://eds.s.ebscohost.com/abstract?site=eds&scope=site&jrnl=22344772&AN=160571615&h=nBWJ8MZ1VBxMHQMYz4s9qQHgOjuLMkVqjCvDvX%2fto3azA7BD2spqKM7vicp7gS4GyrqWjkVPK3G9ZoRdyNUaKA%3d%3d&crl=c&resultLocal=ErrCrlNoResults&resultNs=Ehost&crlhashurl=login.aspx%3fdirect%3dtrue%26profile%3dehost%26scope%3dsite%26authtype%3dcrawler%26jrnl%3d22344772%26AN%3d160571615
 > -> decision tree and random forest classifier, K-means clustering algorithm and manually operated algorithm

**구글 스칼라 인용수 1000회 이상 문헌**

**Recommender systems**
 - https://www.sciencedirect.com/science/article/pii/S0370157312000828
 - 1212회 인용

**Recommender Systems in E-Commerce**
 - https://dl.acm.org/doi/pdf/10.1145/336992.337035
 - 2736회 인용

**Recommender system application developments: A survey**
 - https://www.sciencedirect.com/science/article/pii/S0167923615000627
 - 1717회 인용

**Research-paper recommender systems: a literature survey**
 - https://link.springer.com/article/10.1007/s00799-015-0156-0
 - 1087회 인용
"""

import pandas as pd
import numpy as np
import matplotlib as plt


from tqdm.auto import tqdm
from collections import defaultdict
from sklearn.decomposition import TruncatedSVD, NMF, SparsePCA
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec

df_apply_train = pd.read_csv('/content/apply_train.csv')
df_company = pd.read_csv('/content/company.csv')
df_recruitment = pd.read_csv('/content/recruitment.csv')
df_resume = pd.read_csv('/content/resume.csv')
df_resume_certificate = pd.read_csv('/content/resume_certificate.csv')
df_resume_education = pd.read_csv('/content/resume_education.csv')
df_resume_language = pd.read_csv('/content/resume_language.csv')

print(df_apply_train.shape)
print("이력서 번호의 수 (지원자의 수) :",len(df_apply_train['resume_seq'].unique()))
df_apply_train.head(1)

# resume_seq : 추천을 진행할 이력서 고유 번호 (취준생들의 ID)
# -> 8482명의 지원자들을 가지고 학습 -> 8482의 지원자들이 여러 기업(이력서)를 작성 -> 총 57946개의 이력서

# recreuitment_seq : 이력서에 대해 추천한 채용 공고 고유 번호 (기업들의 ID?)

print(df_company.shape)
print("공고 종류 :", len(df_company['recruitment_seq'].unique()))
# 공고 종류 : 2377
print("회사 종류 :", len(df_company['company_type_seq'].unique()))
# 회사 종류 : 6
print("주 업종 종류 :", len(df_company['supply_kind'].unique()))
# 주 업종 종류 : 17
df_company.head(1)

print(df_recruitment.shape)
df_recruitment.head(1)

print(df_resume.shape)
df_resume.head(1)

print(df_resume_certificate.shape)
df_resume_certificate.head(1)

print(df_resume_education.shape)
df_resume_education.head(1)

print(df_resume_language.shape)
df_resume_language.head(1)

df_recruitment.info()

df_recruitment = df_recruitment[~df_recruitment['address_seq1'].isna()]

df_recruitment.fillna(value=0)

### 데이콘 Baseline code (협업 필터링 코드) ###

# 사용자-아이템 행렬 생성: 구직자가 해당 채용 공고에 지원했으면 1, 아니면 0으로 설정
user_item_matrix = df_apply_train.groupby(['resume_seq', 'recruitment_seq']).size().unstack(fill_value=0)
user_item_matrix[user_item_matrix > 1] = 1
user_item_matrix

# 사용자 간의 유사성 계산
user_similarity = cosine_similarity(user_item_matrix)

# 추천 점수 계산
user_predicted_scores = user_similarity.dot(user_item_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T

# 이미 지원한 채용 공고 제외하고 추천
recommendations = []
for idx, user in enumerate(user_item_matrix.index):
    # 해당 사용자가 지원한 채용 공고
    applied_jobs = set(user_item_matrix.loc[user][user_item_matrix.loc[user] == 1].index)

    # 해당 사용자의 추천 점수 (높은 점수부터 정렬)
    sorted_job_indices = user_predicted_scores[idx].argsort()[::-1]
    recommended_jobs = [job for job in user_item_matrix.columns[sorted_job_indices] if job not in applied_jobs][:5]

    for job in recommended_jobs:
        recommendations.append([user, job])

col_recommend = pd.DataFrame(recommendations)
col_recommend.head(10)

### 콘텐츠 기반 추천 모델링 진행 ###
## 특징 추출에 사용할 df : df_resume, df_resume_certificate, df_resume_education, df_resume_language
##

df_resume.head()

# 모든 컬럼 다 사용, text_keyword, job_code_seq1는 어떻게 처리?

df_resume_certificate.head(1)

# certificate_contents
# Word Toeknizing 진행 bert까지는 안가고 word2vec 까지만 해도 될 듯?

df_resume_education.head()
# hischool_type_seq, hischool_special_type, univ_type_seq1, univ_type_seq2, univ_location, univ_major, univ_sub_major, univ_major_type, univ_score
# major 가 Nan 값인거는 뭐지? -> major 타입도 똑같이 Nan이면 해당 행은 그냥 0으로 유지, major 타입은 있으면 drop
# major 정보의 수가 매우 적음 -> major를 유지하면서 코딩? -> 성능 비교해보자

resume_education_use = df_resume_education[['resume_seq','hischool_type_seq', 'hischool_special_type', 'univ_type_seq1', 'univ_type_seq2', 'univ_location', 'univ_major', 'univ_sub_major', 'univ_major_type', 'univ_score']]

resume_education_use.head(1)

df_resume_language.head(1)

# language, exam_name, score

# resume_use -> resume_need_token들은 token 필요, resume_certificate_use -> token 필요
# resume_education_use, resume_language_use

resume_use.head(1)
resume_need_token = ['text_keyword','job_code_seq1','job_code_seq2','job_code_seq3']

df_resume_certificate = df_resume_certificate.dropna()

result_sum = df_resume_certificate.groupby('resume_seq')['certificate_contents'].agg(', '.join).reset_index()

## 인코딩 진행하기

## model = Word2Vec(sentences=data, vector_size=100, window=5, min_count=1, sg=0)

## df_resume : text_keyword, job_code_seq1, job_code_seq2, job_code_seq3, career_job_code
model = Word2Vec(sentences = df_resume['text_keyword'], vector_size=100, window=5, min_count=1, sg=0)


## df_resume_certificate : certificate_contents



## df_resume_education : hischool_special_type, univ_major, univ_sub_major

result_1 = pd.merge(df_apply_train, result_sum, on='resume_seq', how='left')
result_1.head(1)
## 자격증까지 merge한 상태

result_1.shape

result_1.fillna(0, inplace = True)
result_1.info()

## 나머지 데이터프레임들도 merge하고 싶음

df_resume = df_resume.dropna(subset=['text_keyword'])

df_resume.fillna(0, inplace = True)

# df_resume['job_code_seq2'] = df_resume['job_code_seq2'].fillna(0, inplace=True)
# df_resume['job_code_seq3'] = df_resume['job_code_seq3'].fillna(0, inplace=True)
df_resume.info()

result_2 = pd.merge(result_1, df_resume, on='resume_seq', how='left')
result_2.head(2)

result_3 = pd.merge(result_2, resume_education_use, on='resume_seq', how='left')
result_3.head(2)

result_3.fillna(0, inplace = True)

df_resume_language.head(1)

result_sum = df_resume_certificate.groupby('resume_seq')['certificate_contents'].agg(', '.join).reset_index()

result_4 = pd.merge(result_3, df_resume_language, on='resume_seq', how='left')
result_4.head(2)

result_4.fillna(0, inplace = True)
result_4.info()

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

preprocess = ['text_keyword', 'job_code_seq1', 'job_code_seq2', 'job_code_seq3', 'career_job_code', 'certificate_contents', 'hischool_special_type', 'univ_major', 'univ_sub_major']

result_4[preprocess].head(2)

special_characters = re.compile('[^ ㄱ-ㅎ|ㅏ-ㅣ|가-힣|0-9]+')

for col in preprocess:
    result_4[col] = result_4[col].apply(lambda x: special_characters.sub('', str(x)))

time_df = ['reg_date' ,'updated_date']
result_4[time_df].head(1)

import datetime

result_4['reg_year'] = pd.to_datetime(result_4['reg_date']).dt.year
result_4['reg_month'] = pd.to_datetime(result_4['reg_date']).dt.month
result_4['reg_day'] = pd.to_datetime(result_4['reg_date']).dt.day

result_4['updated_year'] = pd.to_datetime(result_4['updated_date']).dt.year
result_4['updated_month'] = pd.to_datetime(result_4['updated_date']).dt.month
result_4['updated_day'] = pd.to_datetime(result_4['updated_date']).dt.day

result_4 = result_4.drop(columns = ['reg_date','updated_date'])

result_4.info()

tfidf = TfidfVectorizer()

result_4[preprocess].head()

model = Word2Vec(sentences=data, vector_size=100, window=5, min_count=1, sg=0)

